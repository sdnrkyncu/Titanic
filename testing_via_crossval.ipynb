{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31fbf3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10936965981495178\n",
      "0.412136457171825\n",
      "0.10936965981495178\n",
      "0.412136457171825\n",
      "Best parameters: {'fit_intercept': True, 'normalize': True}\n",
      " Best estimator: LinearRegression(normalize=True)\n",
      "{'mean_fit_time': array([0.00154963, 0.00088449, 0.00096264, 0.00081697]), 'std_fit_time': array([6.69774604e-04, 1.77098784e-04, 2.50698178e-04, 3.48191051e-05]), 'mean_score_time': array([0.0003448 , 0.00029545, 0.0003705 , 0.00027075]), 'std_score_time': array([7.75436627e-05, 1.12670415e-04, 1.68504564e-04, 5.54495395e-05]), 'param_fit_intercept': masked_array(data=[True, True, False, False],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_normalize': masked_array(data=[True, False, True, False],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'fit_intercept': True, 'normalize': True}, {'fit_intercept': True, 'normalize': False}, {'fit_intercept': False, 'normalize': True}, {'fit_intercept': False, 'normalize': False}], 'split0_test_score': array([-0.15375341, -0.15375341, -0.15375341, -0.15375341]), 'split1_test_score': array([-0.1597815, -0.1597815, -0.1597815, -0.1597815]), 'split2_test_score': array([-0.16535456, -0.16535456, -0.16535456, -0.16535456]), 'split3_test_score': array([-0.18728075, -0.18728075, -0.18728075, -0.18728075]), 'split4_test_score': array([-0.14218568, -0.14218568, -0.14218568, -0.14218568]), 'mean_test_score': array([-0.16167118, -0.16167118, -0.16167118, -0.16167118]), 'std_test_score': array([0.01493562, 0.01493562, 0.01493562, 0.01493562]), 'rank_test_score': array([1, 1, 1, 1], dtype=int32)}\n",
      "0.402083548341115 {'fit_intercept': True, 'normalize': True}\n",
      "0.402083548341115 {'fit_intercept': True, 'normalize': False}\n",
      "0.402083548341115 {'fit_intercept': False, 'normalize': True}\n",
      "0.402083548341115 {'fit_intercept': False, 'normalize': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [-0.19121393 -0.17085825 -0.16846324 -0.2060136  -0.16744896 -0.16320656\n",
      " -0.19926472 -0.17071084 -0.16558657         nan         nan         nan\n",
      " -0.20834057 -0.19757252 -0.21645843 -0.20273309 -0.20487774 -0.19627007]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_features': 4, 'n_estimators': 30}\n",
      " Best estimator: RandomForestRegressor(max_features=4, n_estimators=30, random_state=42)\n",
      "keys:dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_max_features', 'param_n_estimators', 'param_bootstrap', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score']) \n",
      "\n",
      "\n",
      " {'mean_fit_time': array([0.00400286, 0.01124554, 0.03766532, 0.00429339, 0.0120646 ,\n",
      "       0.03188596, 0.00440521, 0.01261325, 0.046737  , 0.00355997,\n",
      "       0.00571065, 0.01260715, 0.00494022, 0.01581836, 0.00674543,\n",
      "       0.01801662, 0.00570979, 0.02070026]), 'std_fit_time': array([0.00021253, 0.00045776, 0.00573288, 0.00031703, 0.00050448,\n",
      "       0.00041065, 0.00036044, 0.00045406, 0.0087859 , 0.00162734,\n",
      "       0.00169522, 0.00163575, 0.00115806, 0.00256497, 0.00225351,\n",
      "       0.00077569, 0.0018718 , 0.00320712]), 'mean_score_time': array([0.00095897, 0.00129414, 0.00336442, 0.00095224, 0.00136137,\n",
      "       0.00260406, 0.00073752, 0.00132728, 0.00366364, 0.        ,\n",
      "       0.        , 0.        , 0.00169544, 0.00234227, 0.00187378,\n",
      "       0.00234199, 0.00094185, 0.00281873]), 'std_score_time': array([3.39516107e-04, 1.19444424e-04, 6.42688891e-04, 2.96288822e-04,\n",
      "       1.81117479e-04, 1.09786762e-04, 6.28728553e-05, 2.46960266e-04,\n",
      "       1.19525454e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.39626909e-03, 5.05779252e-04, 8.08634498e-04, 1.30618411e-03,\n",
      "       1.11420591e-04, 1.38959832e-03]), 'param_max_features': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 3, 3, 4, 4],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[3, 10, 30, 3, 10, 30, 3, 10, 30, 3, 10, 30, 3, 10, 3,\n",
      "                   10, 3, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_bootstrap': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, False,\n",
      "                   False, False, False, False, False],\n",
      "             mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                    True,  True,  True,  True, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_features': 2, 'n_estimators': 3}, {'max_features': 2, 'n_estimators': 10}, {'max_features': 2, 'n_estimators': 30}, {'max_features': 4, 'n_estimators': 3}, {'max_features': 4, 'n_estimators': 10}, {'max_features': 4, 'n_estimators': 30}, {'max_features': 6, 'n_estimators': 3}, {'max_features': 6, 'n_estimators': 10}, {'max_features': 6, 'n_estimators': 30}, {'max_features': 8, 'n_estimators': 3}, {'max_features': 8, 'n_estimators': 10}, {'max_features': 8, 'n_estimators': 30}, {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}, {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}, {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}, {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}, {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}, {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}], 'split0_test_score': array([-0.21521167, -0.18851865, -0.18388682, -0.24230537, -0.18657552,\n",
      "       -0.17845372, -0.22446616, -0.18623074, -0.17465498,         nan,\n",
      "               nan,         nan, -0.21883556, -0.21232873, -0.21821482,\n",
      "       -0.2069237 , -0.21604226, -0.20596514]), 'split1_test_score': array([-0.19791844, -0.17409023, -0.17079352, -0.19919805, -0.16294204,\n",
      "       -0.16209113, -0.21471689, -0.18093096, -0.1760938 ,         nan,\n",
      "               nan,         nan, -0.21012671, -0.19057077, -0.21718386,\n",
      "       -0.19692377, -0.20400589, -0.19321147]), 'split2_test_score': array([-0.16723734, -0.16114709, -0.15761056, -0.17913653, -0.15490703,\n",
      "       -0.15205273, -0.16590329, -0.14829994, -0.1495575 ,         nan,\n",
      "               nan,         nan, -0.21536881, -0.19343928, -0.22051862,\n",
      "       -0.1996057 , -0.18562782, -0.17553197]), 'split3_test_score': array([-0.20699463, -0.18336334, -0.18544213, -0.24255538, -0.19886389,\n",
      "       -0.19264503, -0.24389295, -0.20340584, -0.19093302,         nan,\n",
      "               nan,         nan, -0.22346395, -0.21995272, -0.23457468,\n",
      "       -0.23478817, -0.24331375, -0.22927162]), 'split4_test_score': array([-0.16870756, -0.14717194, -0.14458319, -0.16687266, -0.13395631,\n",
      "       -0.13079017, -0.14734429, -0.13468674, -0.13669355,         nan,\n",
      "               nan,         nan, -0.17390781, -0.17157113, -0.19180018,\n",
      "       -0.17542412, -0.175399  , -0.17737013]), 'mean_test_score': array([-0.19121393, -0.17085825, -0.16846324, -0.2060136 , -0.16744896,\n",
      "       -0.16320656, -0.19926472, -0.17071084, -0.16558657,         nan,\n",
      "               nan,         nan, -0.20834057, -0.19757252, -0.21645843,\n",
      "       -0.20273309, -0.20487774, -0.19627007]), 'std_test_score': array([0.01975493, 0.01506029, 0.01561818, 0.03147455, 0.02303082,\n",
      "       0.02133342, 0.03653567, 0.02535455, 0.01963068,        nan,\n",
      "              nan,        nan, 0.01775986, 0.01709269, 0.01382985,\n",
      "       0.0191541 , 0.02384235, 0.01989838]), 'rank_test_score': array([ 7,  6,  4, 13,  3,  1, 10,  5,  2, 16, 17, 18, 14,  9, 15, 11, 12,\n",
      "        8], dtype=int32)}\n",
      "0.43728015034129586 {'max_features': 2, 'n_estimators': 3}\n",
      "0.41335003278441307 {'max_features': 2, 'n_estimators': 10}\n",
      "0.4104427410040683 {'max_features': 2, 'n_estimators': 30}\n",
      "0.4538872098919531 {'max_features': 4, 'n_estimators': 3}\n",
      "0.4092052746971811 {'max_features': 4, 'n_estimators': 10}\n",
      "0.40398831168923277 {'max_features': 4, 'n_estimators': 30}\n",
      "0.4463907665247816 {'max_features': 6, 'n_estimators': 3}\n",
      "0.41317168743952215 {'max_features': 6, 'n_estimators': 10}\n",
      "0.4069232982305372 {'max_features': 6, 'n_estimators': 30}\n",
      "nan {'max_features': 8, 'n_estimators': 3}\n",
      "nan {'max_features': 8, 'n_estimators': 10}\n",
      "nan {'max_features': 8, 'n_estimators': 30}\n",
      "0.45644338856819855 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "0.44449130913950213 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "0.46525093527493144 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "0.4502589159002453 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "0.4526342284269093 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "0.44302377789198394 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n",
      "feature: Age, importance: 0.29068027967507276\n",
      "feature: Fare, importance: 0.3957278163786874\n",
      "feature: Sex, importance: 0.27474567955709267\n",
      "feature: Embarked, importance: 0.013458922853767189\n",
      "Best parameters: {'max_features': 4, 'n_estimators': 24}\n",
      " Best estimator: RandomForestRegressor(max_features=4, n_estimators=24, random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "35 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan         nan         nan         nan\n",
      " -0.16499069         nan -0.16199528 -0.18770236]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%run main.ipynb\n",
    "%run hyperparameter_optimization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f1094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib # to save the results of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f239cf",
   "metadata": {},
   "source": [
    "### Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c441411",
   "metadata": {},
   "source": [
    "Scikit-Learn’s **K-fold cross-validation** feature: randomly splits the training set into 10 distinct subsets called folds, <br>\n",
    "then it trains and evaluates the Decision Tree model 10 times,<br>\n",
    "picking a different fold for evaluation every time <br>\n",
    "and training on the other 9 folds.<br> \n",
    "<br>\n",
    "The result is an array containing the 10 evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b84e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dce99",
   "metadata": {},
   "source": [
    "**Evaluation for LinearRegregression model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43005cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_scores = cross_val_score(tree_reg, titanic_clean, \n",
    "                         titanic_train_labels, scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7c750",
   "metadata": {},
   "source": [
    "PS: Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than\n",
    "a cost function (lower is better), so the scoring function is actually the opposite of the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb22f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_scores_rms = np.sqrt(-lin_reg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776cb3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49903535, 0.41670786, 0.5421875 , 0.50058566, 0.4719372 ,\n",
       "       0.38049347, 0.45059004, 0.47107336, 0.44619661, 0.43615474])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_scores_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314fcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -0.21489988828448564\n",
      "std: 0.0405210205666955\n"
     ]
    }
   ],
   "source": [
    "# estimate of the performance of your model\n",
    "print(\"mean:\", lin_reg_scores.mean())\n",
    "# how precise this estimate is\n",
    "print(\"std:\", lin_reg_scores.std())\n",
    "\n",
    "# You would not have this information if you just used one validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6882fe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46 accuracy with a standard deviation of 0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\"\n",
    "      % (lin_reg_scores_rms.mean(), lin_reg_scores_rms.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "172149c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(lin_rms_scores, \"lin_reg.pkl\")\n",
    "#my_model_loaded = joblib.load(\"lin_reg.pkl\")\n",
    "#my_model_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792a7e0",
   "metadata": {},
   "source": [
    "Evaluation with the best parameters obtained from hyperparamater_optimization.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000c404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lin_reg_grid_search = lin_reg_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfc508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16015625, 0.8671875 , 0.66015625, 0.70703125, 0.140625  ,\n",
       "       0.140625  , 0.16796875, 0.20703125, 0.6640625 , 0.8515625 ,\n",
       "       0.70703125, 0.6328125 , 0.1640625 , 0.1640625 , 0.6796875 ,\n",
       "       0.62890625, 0.20703125, 0.16015625, 0.6640625 , 0.796875  ,\n",
       "       0.1640625 , 0.1484375 , 0.66796875, 0.1875    , 0.70703125,\n",
       "       0.671875  , 0.2890625 , 0.49609375, 0.6484375 , 0.15234375,\n",
       "       0.296875  , 0.98046875, 0.6484375 , 0.09375   , 0.38671875,\n",
       "       0.1875    , 0.2890625 , 0.16015625, 0.6875    , 0.82421875,\n",
       "       0.640625  , 0.67578125, 0.2890625 , 0.8828125 , 0.66015625,\n",
       "       0.15234375, 0.1484375 , 0.6484375 , 0.30859375, 0.6875    ,\n",
       "       0.22265625, 0.16015625, 0.85546875, 0.6796875 , 0.30078125,\n",
       "       0.1875    , 0.671875  , 0.2890625 , 0.72265625, 0.2265625 ,\n",
       "       0.30078125, 0.984375  , 0.22265625, 0.21484375, 0.31640625,\n",
       "       0.30078125, 0.66015625, 0.1640625 , 0.67578125, 0.15234375,\n",
       "       0.1484375 , 0.7265625 , 0.24609375, 0.30078125, 0.20703125,\n",
       "       0.15625   , 0.15234375, 0.15234375, 0.22265625, 0.66015625,\n",
       "       0.16015625, 0.1484375 , 0.6484375 , 0.203125  , 0.6796875 ,\n",
       "       0.6640625 , 0.203125  , 0.15234375, 1.        , 0.15625   ,\n",
       "       0.1484375 , 0.1640625 , 0.19140625, 0.16796875, 0.1015625 ,\n",
       "       0.15234375, 0.2578125 , 0.37109375, 0.66796875, 0.1640625 ,\n",
       "       0.66015625, 0.15234375, 0.25      , 0.14453125, 0.13671875,\n",
       "       0.15234375, 0.66796875, 0.15234375, 0.1328125 , 0.66796875,\n",
       "       0.1796875 , 0.828125  , 0.16015625, 0.67578125, 0.82421875,\n",
       "       0.16015625, 0.0703125 , 0.1640625 , 0.609375  , 0.73046875,\n",
       "       0.24609375, 0.15234375, 0.3125    , 0.66015625, 0.19921875,\n",
       "       0.3203125 , 0.140625  , 0.15625   , 0.81640625, 0.125     ,\n",
       "       0.28125   , 0.1640625 , 0.63671875, 0.6796875 , 0.1640625 ,\n",
       "       0.30859375, 0.6953125 , 0.1953125 , 0.16796875, 0.390625  ,\n",
       "       0.80859375, 0.66796875, 0.67578125, 0.15234375, 0.171875  ,\n",
       "       0.203125  , 0.15234375, 0.72265625, 0.16015625, 0.13671875,\n",
       "       0.1171875 , 0.7421875 , 0.10546875, 0.13671875, 0.15234375,\n",
       "       0.32421875, 0.6640625 , 0.1484375 , 0.15234375, 0.23046875,\n",
       "       0.13671875, 0.65234375, 0.15234375, 0.16796875, 0.234375  ,\n",
       "       0.1953125 , 0.72265625, 0.66015625, 0.17578125, 0.21484375,\n",
       "       0.12890625, 0.203125  , 0.70703125, 0.16015625, 0.27734375,\n",
       "       0.16796875, 0.17578125, 0.7890625 , 0.15625   , 0.125     ,\n",
       "       0.73828125, 0.30078125, 0.2109375 , 0.234375  , 0.71484375,\n",
       "       0.20703125, 0.65625   , 0.1484375 , 0.12890625, 0.13671875,\n",
       "       0.66015625, 0.171875  , 0.671875  , 0.21484375, 0.80078125,\n",
       "       0.9296875 , 0.140625  , 0.12890625, 0.6484375 , 0.671875  ,\n",
       "       0.15234375, 0.23046875, 0.13671875, 0.26171875, 0.16796875,\n",
       "       0.703125  , 0.15625   , 0.30859375, 0.6640625 , 0.30078125,\n",
       "       0.15625   , 0.6640625 , 0.16015625, 0.15625   , 0.140625  ,\n",
       "       0.9296875 , 0.66015625, 0.15234375, 0.8828125 , 0.15234375,\n",
       "       0.16796875, 0.16015625, 0.11328125, 0.15234375, 0.23828125,\n",
       "       0.16015625, 0.16796875, 0.1640625 , 0.17578125, 0.68359375,\n",
       "       0.74609375, 0.1484375 , 0.109375  , 0.7265625 , 0.16015625,\n",
       "       0.66015625, 0.1484375 , 0.71484375, 0.16796875, 0.1484375 ,\n",
       "       0.80859375, 0.65625   , 0.15234375, 0.16015625, 0.28515625,\n",
       "       0.21875   , 0.6640625 , 0.671875  , 0.1953125 , 0.1328125 ,\n",
       "       0.15234375, 0.66015625, 0.12109375, 0.16015625, 0.65234375,\n",
       "       0.8046875 , 0.890625  , 0.7578125 , 1.4453125 , 0.6484375 ,\n",
       "       0.140625  , 0.22265625, 0.20703125, 0.12109375, 0.6484375 ,\n",
       "       0.140625  , 0.20703125, 0.15625   , 0.796875  , 0.81640625,\n",
       "       0.18359375, 0.14453125, 0.65234375, 0.3046875 , 0.6484375 ,\n",
       "       0.69140625, 0.6328125 , 0.140625  , 0.19921875, 0.6640625 ,\n",
       "       0.078125  , 0.15234375, 0.16796875, 0.1640625 , 0.17578125,\n",
       "       0.28515625, 0.1484375 , 0.16015625, 0.13671875, 0.65625   ,\n",
       "       0.75390625, 0.921875  , 0.28515625, 0.6640625 , 0.15625   ,\n",
       "       0.31640625, 0.296875  , 0.88671875, 0.1796875 , 1.07421875,\n",
       "       0.6484375 , 0.16015625, 0.15234375, 0.65234375, 0.15234375,\n",
       "       0.3828125 , 0.93359375, 0.94921875, 0.30859375, 0.859375  ,\n",
       "       0.90234375, 1.14453125, 0.68359375, 0.15234375, 0.1484375 ,\n",
       "       0.66015625, 0.6875    , 0.1171875 , 0.85546875, 0.9453125 ,\n",
       "       0.16015625, 0.15234375, 0.6484375 , 0.6953125 , 0.23046875,\n",
       "       0.953125  , 0.09375   , 0.65234375, 0.66796875, 0.8828125 ,\n",
       "       0.66796875, 0.1484375 , 0.3203125 , 0.1796875 , 0.82421875,\n",
       "       0.15234375, 0.22265625, 0.94140625, 0.125     , 0.16015625,\n",
       "       0.21484375, 0.99609375, 0.16015625, 0.1640625 , 0.14453125,\n",
       "       0.671875  , 0.6484375 , 0.671875  , 0.203125  , 0.12890625,\n",
       "       0.16015625, 0.1875    , 0.3125    , 0.16796875, 0.2890625 ,\n",
       "       0.15234375, 0.73046875, 0.6484375 , 0.6484375 , 0.6484375 ,\n",
       "       0.16015625, 0.31640625, 0.78125   , 0.140625  , 0.1484375 ,\n",
       "       0.1484375 , 0.8359375 , 0.796875  , 0.6484375 , 0.88671875,\n",
       "       0.359375  , 0.1640625 , 0.1640625 , 0.46875   , 0.71484375,\n",
       "       0.89453125, 0.66796875, 0.55859375, 0.296875  , 0.1640625 ,\n",
       "       1.0625    , 0.8515625 , 0.14453125, 0.70703125, 0.15234375,\n",
       "       0.25390625, 0.24609375, 0.65234375, 0.140625  , 0.8203125 ,\n",
       "       0.28125   , 0.16015625, 0.15234375, 0.9453125 , 0.67578125,\n",
       "       0.16015625, 0.65234375, 0.14453125, 0.1640625 , 0.6640625 ,\n",
       "       0.1328125 , 0.15234375, 0.671875  , 0.1640625 , 0.671875  ,\n",
       "       0.15625   , 0.11328125, 0.20703125, 0.16015625, 0.68359375,\n",
       "       0.15234375, 0.140625  , 0.74609375, 0.140625  , 0.125     ,\n",
       "       0.66015625, 0.6796875 , 0.68359375, 0.15625   , 0.70703125,\n",
       "       0.2890625 , 0.1484375 , 0.1484375 , 0.66796875, 0.18359375,\n",
       "       0.15234375, 0.68359375, 0.6953125 , 0.140625  , 0.14453125,\n",
       "       0.17578125, 0.671875  , 0.66015625, 0.16796875, 0.1796875 ,\n",
       "       0.82421875, 0.703125  , 0.6796875 , 0.42578125, 0.1484375 ,\n",
       "       0.65625   , 0.1640625 , 0.15625   , 0.66796875, 0.15234375,\n",
       "       0.28515625, 0.69921875, 0.1640625 , 0.8515625 , 0.140625  ,\n",
       "       0.1640625 , 0.16796875, 0.3125    , 0.36328125, 0.15234375,\n",
       "       0.2890625 , 0.11328125, 0.71875   , 0.62890625, 0.140625  ,\n",
       "       0.140625  , 0.140625  , 0.16015625, 0.125     , 0.15234375,\n",
       "       0.1328125 , 0.140625  , 0.12890625, 0.140625  , 0.85546875,\n",
       "       0.15234375, 0.1328125 , 0.6796875 , 0.8125    , 0.671875  ,\n",
       "       0.2109375 , 0.15625   , 0.1484375 , 0.16015625, 0.703125  ,\n",
       "       0.23046875, 0.140625  , 0.1171875 , 0.6015625 , 0.40234375,\n",
       "       0.68359375, 0.75390625, 0.2734375 , 0.1484375 , 0.19140625,\n",
       "       0.16796875, 0.16015625, 0.13671875, 0.27734375, 0.16015625,\n",
       "       0.30078125, 0.84765625, 0.16015625, 0.8515625 , 0.15625   ,\n",
       "       0.16796875, 0.65625   , 0.6484375 , 0.64453125, 0.77734375,\n",
       "       0.4375    , 0.67578125, 0.17578125, 0.171875  , 0.21484375,\n",
       "       0.13671875, 0.15234375, 0.16015625, 0.82421875, 0.15625   ,\n",
       "       0.15625   , 0.65234375, 0.16015625, 0.66796875, 0.14453125,\n",
       "       0.765625  , 0.16015625, 0.2890625 , 0.83984375, 0.2890625 ,\n",
       "       0.1171875 , 0.62890625, 0.4296875 , 0.1328125 , 0.1640625 ,\n",
       "       0.72265625, 0.2890625 , 0.30859375, 0.81640625, 0.65625   ,\n",
       "       0.71484375, 0.1484375 , 0.92578125, 0.16015625, 0.86328125,\n",
       "       0.7265625 , 0.71875   , 0.71484375, 0.16796875, 0.3828125 ,\n",
       "       0.1171875 , 0.6953125 , 0.296875  , 0.16015625, 0.22265625,\n",
       "       0.44140625, 0.17578125, 0.140625  , 0.30078125, 0.66796875,\n",
       "       0.12109375, 0.80859375, 0.578125  , 0.734375  , 0.65625   ,\n",
       "       0.140625  , 0.1328125 , 0.16015625, 0.15234375, 0.66015625,\n",
       "       0.17578125, 0.1640625 , 0.671875  , 0.2890625 , 0.14453125,\n",
       "       0.1015625 , 0.671875  , 0.16015625, 0.6484375 , 0.16796875,\n",
       "       0.171875  , 0.65625   , 0.703125  , 0.80859375, 0.14453125,\n",
       "       0.69140625, 0.9140625 , 0.1328125 , 0.3203125 , 0.29296875,\n",
       "       0.76953125, 0.12890625, 0.33203125, 0.16015625, 0.15234375,\n",
       "       0.140625  , 0.8515625 , 0.12109375, 0.6484375 , 0.16015625,\n",
       "       0.15625   , 0.69140625, 0.10546875, 0.2890625 , 0.3203125 ,\n",
       "       0.6875    , 0.15234375, 0.1953125 , 0.125     , 0.3046875 ,\n",
       "       0.14453125, 0.1484375 , 0.1796875 , 0.8515625 , 0.828125  ,\n",
       "       0.671875  , 0.15234375, 0.65625   , 0.140625  , 0.140625  ,\n",
       "       0.73828125, 0.1484375 , 0.671875  , 0.734375  , 0.15625   ,\n",
       "       0.30078125, 0.1875    , 0.3125    , 0.16015625, 0.171875  ,\n",
       "       0.12890625, 0.09765625, 0.7578125 , 0.15234375, 0.140625  ,\n",
       "       0.09375   , 0.11328125, 0.3125    , 0.140625  , 0.71484375,\n",
       "       0.66796875, 0.14453125, 0.16796875, 0.67578125, 0.1640625 ,\n",
       "       0.1640625 , 0.88671875, 0.7265625 , 0.21484375, 0.85546875,\n",
       "       0.34765625, 0.1640625 , 0.28125   , 0.15234375, 0.66796875,\n",
       "       0.15234375, 0.6953125 , 0.16015625, 0.6484375 , 0.6640625 ,\n",
       "       0.2421875 , 0.15234375, 0.6484375 , 0.16796875, 0.37890625,\n",
       "       0.28125   , 0.26953125, 0.14453125, 0.13671875, 0.1640625 ,\n",
       "       0.23046875, 0.1640625 , 0.15234375, 0.125     , 0.71875   ,\n",
       "       0.6796875 , 0.203125  , 0.0859375 , 0.15234375, 0.140625  ,\n",
       "       0.16796875, 0.15625   , 0.6796875 , 0.68359375, 0.93359375,\n",
       "       0.6484375 , 0.3828125 , 0.1640625 , 0.22265625, 0.140625  ,\n",
       "       0.33984375, 0.2109375 , 0.16796875, 0.16796875, 0.9453125 ,\n",
       "       0.20703125, 0.84375   , 0.21484375, 0.29296875, 0.125     ,\n",
       "       0.12109375, 0.125     , 0.6484375 , 0.390625  , 0.12890625,\n",
       "       1.1015625 , 0.1640625 , 0.82421875, 0.14453125, 0.15234375,\n",
       "       0.15625   , 0.640625  , 0.15234375, 0.85546875, 0.30078125,\n",
       "       0.859375  , 0.17578125, 0.17578125, 0.1484375 , 0.12109375,\n",
       "       0.1640625 , 1.0703125 , 0.6640625 , 0.1484375 , 0.14453125,\n",
       "       0.7265625 , 0.16796875, 0.1484375 , 0.125     , 0.2109375 ,\n",
       "       0.1640625 , 0.671875  , 0.6484375 , 0.1796875 , 0.6640625 ,\n",
       "       0.921875  , 0.33203125, 0.140625  , 0.16796875, 0.16796875,\n",
       "       0.16015625, 0.66015625, 0.9375    , 0.15234375, 0.15234375,\n",
       "       0.1796875 , 0.23046875, 1.140625  , 0.16796875, 0.14453125,\n",
       "       0.1640625 , 0.18359375, 0.6640625 , 0.22265625, 0.1328125 ,\n",
       "       0.71484375, 0.19140625, 0.14453125, 0.16015625, 0.69921875,\n",
       "       0.203125  , 0.15234375, 0.171875  , 0.140625  , 0.75390625,\n",
       "       0.16015625, 0.12890625, 0.30078125, 0.7890625 , 0.16796875,\n",
       "       0.7109375 , 0.33203125, 0.64453125, 0.16015625, 0.14453125,\n",
       "       0.15625   , 0.1171875 , 0.6171875 , 0.2890625 , 0.63671875,\n",
       "       0.16796875, 0.140625  , 0.69921875, 0.140625  , 0.8984375 ,\n",
       "       0.8203125 , 0.73828125, 0.17578125, 0.171875  , 0.15625   ,\n",
       "       0.15625   , 0.67578125, 0.19921875, 0.2109375 , 0.35546875,\n",
       "       0.140625  , 0.19140625, 0.73828125, 0.3203125 , 0.15625   ,\n",
       "       0.140625  , 0.6484375 , 0.65234375, 0.28515625, 0.67578125,\n",
       "       0.1484375 , 0.67578125, 0.3203125 , 0.3359375 , 0.15234375,\n",
       "       0.14453125, 0.12109375, 0.67578125, 0.140625  , 0.7109375 ,\n",
       "       0.15234375, 0.15234375, 0.14453125, 0.7265625 , 0.1484375 ,\n",
       "       0.140625  , 0.66796875, 0.32421875, 0.12109375, 0.20703125,\n",
       "       0.73046875, 0.15234375, 0.12109375, 0.6640625 , 0.23046875,\n",
       "       0.140625  , 0.21484375, 0.37109375, 0.140625  , 0.94921875,\n",
       "       0.828125  , 0.2109375 , 0.2890625 , 0.16015625, 0.16796875,\n",
       "       0.87890625, 0.16015625, 0.15234375, 0.20703125, 0.3203125 ,\n",
       "       0.1640625 , 0.171875  , 0.82421875, 0.27734375, 0.16796875,\n",
       "       0.12890625, 0.23046875, 0.28125   , 0.18359375, 0.90625   ,\n",
       "       0.21875   , 0.078125  , 0.83984375, 0.71484375, 0.65625   ,\n",
       "       0.67578125, 0.8359375 , 0.13671875, 0.8203125 , 0.2890625 ,\n",
       "       0.13671875, 0.1640625 , 0.6484375 , 0.73828125, 0.1640625 ,\n",
       "       0.64453125, 0.80859375, 0.19921875, 0.15234375, 0.19140625,\n",
       "       0.15234375, 0.6875    , 0.140625  , 0.12109375, 0.8203125 ,\n",
       "       0.8203125 , 0.16796875, 0.1640625 , 0.15234375, 0.8515625 ,\n",
       "       0.6875    , 0.14453125, 0.671875  , 0.15625   , 0.15625   ,\n",
       "       0.65625   , 0.16015625, 0.69921875, 0.6796875 , 0.32421875,\n",
       "       0.1328125 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lin_reg_grid_search.predict(titanic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c708c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sudenur/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_lin_reg_grid_search_scores = cross_val_score(best_lin_reg_grid_search, titanic_clean, \n",
    "                         titanic_train_labels, scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7699902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lin_reg_grid_search_scores_rms = np.sqrt(-best_lin_reg_grid_search_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab1a682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39864027, 0.38361078, 0.41871188, 0.3806152 , 0.41901505,\n",
       "       0.39427333, 0.42906318, 0.42567232, 0.34805982, 0.40392333])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lin_reg_grid_search_scores_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3e8703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\"\n",
    "      % (best_lin_reg_grid_search_scores_rms.mean(), best_lin_reg_grid_search_scores_rms.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27926bb6",
   "metadata": {},
   "source": [
    "**Evaluation for DecisionTreeRegressor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a865a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg_scores = cross_val_score(tree_reg, titanic_clean, \n",
    "                         titanic_train_labels, scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99c34e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg_scores_rms = np.sqrt(-tree_reg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0710a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49903535, 0.41670786, 0.5421875 , 0.50058566, 0.4719372 ,\n",
       "       0.38049347, 0.45059004, 0.47107336, 0.44619661, 0.43615474])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg_scores_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d821ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46 accuracy with a standard deviation of 0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\"\n",
    "      % (tree_reg_scores_rms.mean(), tree_reg_scores_rms.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad354a4",
   "metadata": {},
   "source": [
    "**Evaluation for RandomForestRegressor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86278fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg_scores = cross_val_score(tree_reg, titanic_clean, \n",
    "                         titanic_train_labels, scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de346835",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg_scores_rms = np.sqrt(-forest_reg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9141692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49903535, 0.41670786, 0.5421875 , 0.50058566, 0.4719372 ,\n",
       "       0.38049347, 0.45059004, 0.47107336, 0.44619661, 0.43615474])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg_scores_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3699aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46 accuracy with a standard deviation of 0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\"\n",
    "      % (forest_reg_scores_rms.mean(), forest_reg_scores_rms.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8310ac5",
   "metadata": {},
   "source": [
    "Evaluation with the best parameters obtained from hyperparamater_optimization.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a40b9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest_reg_rand_search = forest_reg_rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04035d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.        , 0.875     , 1.        , 0.        ,\n",
       "       0.01570513, 0.08333333, 0.33333333, 0.91666667, 0.75      ,\n",
       "       0.75      , 0.95833333, 0.17361111, 0.08333333, 0.33333333,\n",
       "       1.        , 0.16666667, 0.41909722, 0.29166667, 0.87037037,\n",
       "       0.        , 0.29513889, 0.875     , 0.875     , 0.125     ,\n",
       "       0.79166667, 0.        , 0.08333333, 1.        , 0.        ,\n",
       "       0.16666667, 0.95833333, 0.7922551 , 0.29166667, 0.20833333,\n",
       "       0.125     , 0.13979828, 0.02083333, 0.125     , 0.91666667,\n",
       "       0.08333333, 0.25      , 0.04166667, 0.91666667, 0.83333333,\n",
       "       0.        , 0.        , 0.7922551 , 0.25      , 0.125     ,\n",
       "       0.25      , 0.29166667, 1.        , 0.95833333, 0.04166667,\n",
       "       0.875     , 0.72916667, 0.04007937, 0.875     , 0.04166667,\n",
       "       0.20833333, 0.95833333, 0.08333333, 0.25      , 0.06944444,\n",
       "       1.        , 0.91666667, 0.3125    , 0.83333333, 0.125     ,\n",
       "       0.        , 0.375     , 0.        , 0.04166667, 0.89722222,\n",
       "       0.04166667, 0.        , 0.        , 0.91666667, 0.95833333,\n",
       "       0.        , 0.45833333, 0.91007917, 0.        , 0.95833333,\n",
       "       1.        , 0.        , 0.        , 1.        , 0.04166667,\n",
       "       0.        , 0.04166667, 0.125     , 0.        , 0.        ,\n",
       "       0.        , 0.125     , 0.79166667, 0.83333333, 0.        ,\n",
       "       0.20833333, 0.        , 0.04166667, 0.        , 0.21180556,\n",
       "       0.        , 0.83333333, 0.51736111, 0.        , 0.95833333,\n",
       "       0.16666667, 0.41666667, 0.02083333, 0.        , 0.125     ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.08333333,\n",
       "       0.        , 0.        , 0.29166667, 1.        , 0.        ,\n",
       "       0.75      , 0.09920108, 0.66666667, 0.95833333, 0.04166667,\n",
       "       0.08333333, 0.        , 0.25      , 0.95833333, 0.        ,\n",
       "       0.08333333, 1.        , 0.29166667, 0.06944444, 0.33333333,\n",
       "       0.        , 0.875     , 0.85416667, 0.08333333, 0.02083333,\n",
       "       0.29166667, 0.75      , 0.08333333, 0.        , 0.51656746,\n",
       "       0.        , 1.        , 0.04166667, 0.        , 0.        ,\n",
       "       0.16666667, 0.95833333, 0.04166667, 0.20833333, 0.        ,\n",
       "       0.        , 0.95833333, 0.15972222, 0.04166667, 0.16666667,\n",
       "       0.91666667, 0.91666667, 0.25      , 0.        , 0.4203373 ,\n",
       "       0.        , 0.        , 0.91666667, 0.        , 0.08333333,\n",
       "       0.04166667, 0.        , 0.33333333, 0.04166667, 0.        ,\n",
       "       0.        , 0.16666667, 0.125     , 0.79166667, 0.83333333,\n",
       "       0.        , 0.91666667, 0.43333333, 0.04166667, 0.        ,\n",
       "       1.        , 0.0625    , 0.75      , 0.95833333, 0.75      ,\n",
       "       0.95833333, 0.09920108, 0.        , 0.7922551 , 0.51666667,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.91666667,\n",
       "       0.33333333, 0.08333333, 0.91666667, 0.95833333, 0.41666667,\n",
       "       0.20833333, 1.        , 0.        , 0.04166667, 0.09920108,\n",
       "       1.        , 0.875     , 0.15972222, 1.        , 0.04166667,\n",
       "       0.39236111, 0.04166667, 0.04166667, 0.        , 0.625     ,\n",
       "       0.        , 0.4375    , 0.        , 0.02083333, 0.        ,\n",
       "       1.        , 0.04166667, 0.08333333, 0.875     , 0.        ,\n",
       "       0.375     , 0.        , 0.91666667, 0.4375    , 0.10416667,\n",
       "       0.125     , 0.91666667, 0.04166667, 0.08333333, 0.08333333,\n",
       "       0.16666667, 0.08333333, 0.77083333, 0.91666667, 0.        ,\n",
       "       0.        , 0.20833333, 0.08333333, 0.        , 0.41666667,\n",
       "       0.58333333, 0.91666667, 1.        , 1.        , 0.95833333,\n",
       "       0.09920108, 0.70833333, 0.        , 0.        , 0.7922551 ,\n",
       "       0.        , 0.        , 0.55763889, 1.        , 1.        ,\n",
       "       0.33333333, 0.75      , 0.79166667, 0.20833333, 0.7922551 ,\n",
       "       0.95833333, 0.16666667, 0.        , 0.        , 0.95833333,\n",
       "       0.        , 0.        , 0.02777778, 0.49305556, 0.        ,\n",
       "       0.08333333, 0.75      , 0.        , 0.51656746, 0.85510913,\n",
       "       0.75      , 1.        , 0.16666667, 0.125     , 0.04166667,\n",
       "       0.06944444, 0.06547619, 0.20833333, 0.95833333, 1.        ,\n",
       "       0.7922551 , 0.625     , 0.16666667, 1.        , 0.        ,\n",
       "       0.70833333, 1.        , 1.        , 0.16666667, 1.        ,\n",
       "       1.        , 1.        , 0.45833333, 0.        , 0.16666667,\n",
       "       0.79166667, 1.        , 0.        , 1.        , 1.        ,\n",
       "       0.        , 0.16666667, 0.83333333, 0.875     , 0.        ,\n",
       "       1.        , 0.04166667, 1.        , 0.79166667, 1.        ,\n",
       "       0.95833333, 0.        , 0.25      , 0.        , 0.70833333,\n",
       "       0.        , 0.        , 1.        , 0.75      , 0.08333333,\n",
       "       0.91666667, 1.        , 0.41909722, 0.        , 0.05555556,\n",
       "       0.51666667, 0.91666667, 0.91666667, 1.        , 0.        ,\n",
       "       0.        , 0.25      , 0.125     , 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.41666667, 1.        , 1.        ,\n",
       "       0.04166667, 0.04166667, 0.20833333, 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.87037037, 0.7922551 , 0.95833333,\n",
       "       0.875     , 0.08333333, 0.49305556, 0.08333333, 0.125     ,\n",
       "       0.95833333, 0.95833333, 0.125     , 0.45833333, 0.        ,\n",
       "       1.        , 0.91666667, 0.40545635, 1.        , 0.        ,\n",
       "       0.        , 0.125     , 1.        , 0.        , 1.        ,\n",
       "       0.83333333, 0.625     , 0.09375   , 1.        , 0.875     ,\n",
       "       0.08333333, 0.29166667, 0.        , 0.        , 1.        ,\n",
       "       0.56597222, 0.        , 0.        , 0.        , 0.08333333,\n",
       "       0.        , 0.        , 0.95833333, 0.08333333, 0.        ,\n",
       "       0.        , 0.        , 0.95833333, 0.        , 0.79166667,\n",
       "       0.        , 1.        , 0.85416667, 0.04166667, 0.20833333,\n",
       "       0.04166667, 0.        , 0.        , 0.45833333, 0.04166667,\n",
       "       0.        , 0.75      , 1.        , 0.09920108, 0.64583333,\n",
       "       0.63402778, 0.91666667, 0.83333333, 0.04166667, 0.29166667,\n",
       "       0.95833333, 0.29166667, 0.91666667, 0.08333333, 0.08333333,\n",
       "       0.79166667, 0.        , 0.55763889, 1.        , 0.58333333,\n",
       "       0.875     , 0.70833333, 0.88888889, 0.95833333, 0.70833333,\n",
       "       0.20833333, 0.        , 0.        , 0.625     , 0.        ,\n",
       "       0.66666667, 0.08333333, 0.95833333, 0.875     , 0.09920108,\n",
       "       0.75      , 0.        , 0.08333333, 0.04166667, 0.        ,\n",
       "       0.        , 0.        , 0.125     , 0.        , 1.        ,\n",
       "       0.        , 0.        , 1.        , 0.91666667, 0.04166667,\n",
       "       0.04166667, 0.        , 0.        , 0.        , 0.875     ,\n",
       "       0.125     , 0.        , 0.04166667, 0.70833333, 0.75      ,\n",
       "       0.        , 1.        , 0.125     , 0.04166667, 0.95833333,\n",
       "       0.        , 0.        , 0.16666667, 0.04166667, 0.02083333,\n",
       "       0.        , 1.        , 0.        , 0.5       , 0.08333333,\n",
       "       0.04166667, 0.44365079, 0.24537037, 0.125     , 1.        ,\n",
       "       0.08333333, 1.        , 0.63402778, 0.04166667, 0.90972222,\n",
       "       0.74126984, 0.        , 0.95833333, 0.95833333, 0.04166667,\n",
       "       0.04166667, 1.        , 0.08333333, 1.        , 0.06944444,\n",
       "       1.        , 0.        , 0.        , 1.        , 0.13979828,\n",
       "       0.        , 0.875     , 0.08333333, 0.56597222, 0.        ,\n",
       "       0.79166667, 0.13979828, 0.16666667, 0.95833333, 0.125     ,\n",
       "       1.        , 0.43333333, 1.        , 0.        , 0.95833333,\n",
       "       0.95833333, 0.08333333, 0.08333333, 0.66666667, 0.04166667,\n",
       "       0.08333333, 1.        , 0.75      , 0.        , 0.75      ,\n",
       "       0.5       , 0.04166667, 0.02305807, 0.54166667, 0.625     ,\n",
       "       0.08333333, 0.625     , 0.08333333, 1.        , 0.91666667,\n",
       "       0.09920108, 0.10416667, 0.11111111, 0.        , 0.        ,\n",
       "       0.        , 0.0625    , 0.33333333, 0.13979828, 0.64583333,\n",
       "       0.58333333, 0.91666667, 1.        , 0.7922551 , 0.39236111,\n",
       "       0.        , 1.        , 1.        , 0.08333333, 0.40545635,\n",
       "       0.91666667, 1.        , 0.        , 0.08333333, 0.16666667,\n",
       "       0.95833333, 0.        , 0.58333333, 0.02083333, 0.        ,\n",
       "       0.04166667, 1.        , 0.        , 0.7922551 , 0.        ,\n",
       "       0.        , 0.875     , 0.        , 0.        , 0.70833333,\n",
       "       1.        , 0.        , 0.04166667, 0.04166667, 0.58333333,\n",
       "       0.02083333, 0.        , 1.        , 0.875     , 0.95833333,\n",
       "       0.25      , 0.        , 0.91666667, 0.09920108, 0.        ,\n",
       "       1.        , 0.03125   , 0.29166667, 0.91666667, 0.        ,\n",
       "       0.04166667, 0.83333333, 0.95833333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.95833333, 0.        , 0.        ,\n",
       "       0.70833333, 0.        , 0.66666667, 0.        , 0.16666667,\n",
       "       1.        , 0.40545635, 0.33333333, 0.375     , 0.        ,\n",
       "       0.04166667, 0.95833333, 0.125     , 0.4203373 , 1.        ,\n",
       "       0.75      , 0.0625    , 0.625     , 0.01388889, 0.83333333,\n",
       "       0.        , 0.70833333, 0.04166667, 1.        , 0.16666667,\n",
       "       0.04166667, 0.        , 0.25      , 0.        , 0.16666667,\n",
       "       0.625     , 0.        , 0.        , 0.        , 0.4625    ,\n",
       "       0.125     , 0.        , 0.51736111, 0.04166667, 0.95833333,\n",
       "       0.625     , 0.        , 0.125     , 0.70833333, 0.        ,\n",
       "       0.        , 0.04166667, 0.75      , 0.20833333, 0.875     ,\n",
       "       0.375     , 0.83333333, 0.        , 0.        , 0.04166667,\n",
       "       0.25      , 0.        , 0.09722222, 0.08333333, 0.95833333,\n",
       "       0.7875    , 0.875     , 0.4203373 , 0.        , 0.08333333,\n",
       "       0.        , 0.04166667, 0.98346561, 0.        , 0.        ,\n",
       "       1.        , 0.95833333, 0.16666667, 0.02083333, 0.        ,\n",
       "       0.        , 0.83333333, 0.91666667, 0.91666667, 1.        ,\n",
       "       1.        , 0.63402778, 0.58333333, 0.20833333, 0.        ,\n",
       "       0.        , 1.        , 1.        , 0.        , 0.04166667,\n",
       "       0.70833333, 0.        , 0.29513889, 0.        , 0.83333333,\n",
       "       0.04166667, 0.79166667, 0.98346561, 0.        , 0.125     ,\n",
       "       1.        , 0.20833333, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.20833333, 0.875     , 0.        , 0.        ,\n",
       "       0.91666667, 0.08333333, 1.        , 0.        , 0.70267857,\n",
       "       0.        , 0.04166667, 0.95833333, 0.04166667, 0.125     ,\n",
       "       0.91666667, 1.        , 0.08333333, 0.        , 0.95833333,\n",
       "       0.95833333, 0.16666667, 0.02083333, 0.        , 1.        ,\n",
       "       0.        , 0.04166667, 0.70833333, 1.        , 0.0625    ,\n",
       "       1.        , 0.        , 0.16818783, 0.08333333, 0.125     ,\n",
       "       0.        , 0.04166667, 0.45833333, 0.        , 0.875     ,\n",
       "       0.        , 0.09920108, 0.95833333, 0.        , 1.        ,\n",
       "       0.95833333, 0.95833333, 0.33333333, 0.08333333, 0.08333333,\n",
       "       0.        , 0.91666667, 0.        , 0.70833333, 0.25      ,\n",
       "       0.09920108, 0.        , 0.        , 0.25      , 0.04166667,\n",
       "       0.04166667, 1.        , 0.5       , 0.125     , 0.20833333,\n",
       "       0.29513889, 1.        , 0.875     , 0.80555556, 0.70833333,\n",
       "       0.14583333, 0.04166667, 0.25      , 0.04166667, 1.        ,\n",
       "       0.        , 0.        , 0.05208333, 0.20833333, 0.04166667,\n",
       "       0.        , 0.29166667, 0.04166667, 0.        , 0.04166667,\n",
       "       1.        , 0.70833333, 0.        , 0.91666667, 0.125     ,\n",
       "       0.        , 0.4203373 , 0.875     , 0.09920108, 0.95833333,\n",
       "       0.75      , 0.95833333, 0.13979828, 0.        , 0.16666667,\n",
       "       1.        , 0.04166667, 0.        , 0.89722222, 0.75      ,\n",
       "       0.4625    , 0.09722222, 1.        , 0.        , 0.04166667,\n",
       "       0.        , 0.        , 0.08333333, 0.08333333, 1.        ,\n",
       "       0.20833333, 0.        , 0.08333333, 0.79166667, 0.41666667,\n",
       "       0.83333333, 1.        , 0.86111111, 0.91666667, 0.13979828,\n",
       "       0.01388889, 0.        , 0.83333333, 0.        , 0.        ,\n",
       "       0.95833333, 0.91666667, 0.        , 0.        , 0.95833333,\n",
       "       0.        , 0.91666667, 0.04166667, 0.04166667, 0.95833333,\n",
       "       1.        , 0.        , 0.0625    , 0.        , 1.        ,\n",
       "       0.95833333, 0.04166667, 0.22916667, 0.02083333, 0.08333333,\n",
       "       0.08333333, 0.04166667, 0.95833333, 0.16666667, 0.83333333,\n",
       "       0.04166667])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_forest_reg_rand_search.predict(titanic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "677d1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest_reg_rand_search_scores = cross_val_score(best_forest_reg_rand_search, titanic_clean, \n",
    "                         titanic_train_labels, scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a3ff2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4241263 , 0.38363993, 0.44114362, 0.38194328, 0.40094335,\n",
       "       0.3505942 , 0.41721148, 0.43572322, 0.34843388, 0.36757544])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_forest_reg_rand_search_scores_rms = np.sqrt(-best_forest_reg_rand_search_scores)\n",
    "best_forest_reg_rand_search_scores_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05417faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40 accuracy with a standard deviation of 0.03\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\"\n",
    "      % (best_forest_reg_rand_search_scores_rms.mean(), best_forest_reg_rand_search_scores_rms.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f85619",
   "metadata": {},
   "source": [
    "## The best performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "840dcf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [lin_reg, tree_reg, forest_reg, \n",
    "              best_lin_reg_grid_search, best_forest_reg_rand_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8335562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_rms = [lin_reg_scores_rms, tree_reg_scores_rms, forest_reg_scores_rms, \n",
    "              best_lin_reg_grid_search_scores_rms, best_forest_reg_rand_search_scores_rms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef39483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_means = [format(score.mean(), \".2f\") for score in all_scores_rms]\n",
    "all_stds = [format(score.std(), \".2f\") for score in all_scores_rms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2bb6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() 0.46 accuracy with a standard deviation of 0.04\n",
      "DecisionTreeRegressor(random_state=42) 0.46 accuracy with a standard deviation of 0.04\n",
      "RandomForestRegressor(random_state=42) 0.46 accuracy with a standard deviation of 0.04\n",
      "LinearRegression(normalize=True) 0.40 accuracy with a standard deviation of 0.02\n",
      "RandomForestRegressor(max_features=4, n_estimators=24, random_state=42) 0.40 accuracy with a standard deviation of 0.03\n"
     ]
    }
   ],
   "source": [
    "for model, score, std in zip(all_models, all_means, all_stds):\n",
    "    print(model, score, \"accuracy with a standard deviation of\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21f2e308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best model according to the highest accuracy \n",
    "highest_accuracy_idx = all_means.index(max(all_means))\n",
    "best_model = all_models[highest_accuracy_idx]\n",
    "best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
